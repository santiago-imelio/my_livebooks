<!-- livebook:{"file_entries":[{"name":"Apple_Stock_Prices_1981_to_2023.csv","type":"attachment"},{"name":"apple_prices.png","type":"attachment"},{"name":"normalized_apple_prices.csv","type":"attachment"},{"name":"predictions.png","type":"attachment"}],"persist_outputs":true} -->

# Time Series Forecasting with Axon

```elixir
Mix.install([
  {:axon, "~> 0.6.1"},
  {:kino_vega_lite, "~> 0.1.13"},
  {:kino_explorer, "~> 0.1.20"},
  {:nx, "~> 0.7.3"},
  {:tucan, "~> 0.3.1"},
  {:polaris, "~> 0.1.0"},
  {:exla, "~> 0.7.3"},
  {:req, "~> 0.5.2"}
])

Nx.global_default_backend(EXLA.Backend)
Nx.Defn.global_default_options(compiler: EXLA)
```

## Load the dataset

```elixir
data_url =
  "https://raw.githubusercontent.com/santiago-imelio/my_livebooks/main/apple_stock_prices/files/normalized_apple_prices.csv"

raw_csv = Req.get!(data_url).body
raw_data = Regex.split(~r/\r|\n|\r\n/, raw_csv)
```

<!-- livebook:{"output":true} -->

```
["-7.006233945278093067e-01", "-8.208848439047651269e-01", "-9.393830524103554680e-01",
 "-9.471651967287715301e-01", "-6.878552698496820383e-01", "-8.432590195361995278e-01",
 "-8.053201783108936418e-01", "-8.205807307732939648e-01", "-9.202312393102278776e-01",
 "-1.000000000000000000e+00", "-9.881443849549484959e-01", "-8.596141093220164286e-01",
 "-8.706188046502583155e-01", "-9.266151162107476580e-01", "-8.011858533422127060e-01",
 "-7.628820447433897201e-01", "-6.649947770778212863e-01", "-6.728988244388007800e-01",
 "-6.822011452976286527e-01", "-5.421189968740289800e-01", "-4.650859167780025949e-01",
 "-2.148959182702663284e-01", "-1.702082327709231180e-01", "-8.247456439817346663e-02",
 "6.411336396851030628e-02", "8.575760485291095137e-02", "3.860465375596104920e-01",
 "3.946803372690568246e-01", "4.070833060951475346e-01", "5.548260727516387547e-01",
 "4.571211975735445243e-01", "2.172670023675826734e-01", "3.825809222188305547e-01",
 "1.618787285915157526e-01", "1.683843200560799502e-01", "-2.279982092837862240e-03",
 "2.190304256375150693e-01", "1.618787285915157526e-01", "3.212949002041503022e-01",
 "2.193948397353775448e-01", "2.579419011436732134e-01", "3.031162746605300384e-01",
 "4.281805640755953490e-01", "4.270862152306991177e-01", "3.619089325782907096e-01",
 "3.407511912328837766e-01", "5.010795016015081593e-01", "5.381670643928226916e-01",
 "7.000153628233394265e-01", "8.822922096064025332e-01", ...]
```

```elixir
data =
  raw_data
  |> List.pop_at(-1)
  |> elem(1)
  |> Enum.map(&Float.parse/1)
  |> Enum.map(&elem(&1, 0))
  |> Nx.tensor()
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[138]
  EXLA.Backend<host:0, 0.2552156563.1001783322.58341>
  [-0.7006233930587769, -0.8208848237991333, -0.9393830299377441, -0.9471651911735535, -0.6878552436828613, -0.8432590365409851, -0.8053202033042908, -0.8205807209014893, -0.9202312231063843, -1.0, -0.9881443977355957, -0.8596141338348389, -0.8706188201904297, -0.9266151189804077, -0.8011858463287354, -0.7628820538520813, -0.6649947762489319, -0.6728988289833069, -0.6822011470794678, -0.542119026184082, -0.4650859236717224, -0.21489591896533966, -0.17020823061466217, -0.08247456699609756, 0.06411336362361908, 0.08575760573148727, 0.38604652881622314, 0.39468035101890564, 0.40708330273628235, 0.5548260807991028, 0.45712119340896606, 0.2172670066356659, 0.3825809359550476, 0.16187873482704163, 0.16838431358337402, -0.002279982203617692, 0.21903042495250702, 0.16187873482704163, 0.321294903755188, 0.21939483284950256, 0.2579419016838074, 0.3031162619590759, 0.4281805753707886, 0.42708620429039, 0.3619089424610138, 0.34075120091438293, 0.5010794997215271, 0.5381670594215393, 0.7000153660774231, 0.8822922110557556, ...]
>
```

```elixir
plt_domain = Nx.linspace(0, 137, n: 137, type: {:u, 8})

plt_data = [
  prices: data,
  t: plt_domain
]

Tucan.lineplot(plt_data, "t", "prices", height: 400, width: 600, line_color: "black")

# Comment to visualize with KinoVegaLite
:ok
```

<!-- livebook:{"output":true} -->

```
:ok
```

![](files/apple_prices.png)

## Sliding window

```elixir
sliding_window = fn series, win_size ->
  {p} = Nx.shape(series)

  x_idx =
    series
    |> Nx.shape()
    |> Nx.iota()
    |> Nx.reshape({:auto, 1})
    |> Nx.vectorize(:elements)

  y_idx =
    series
    |> Nx.shape()
    |> Nx.iota()
    |> Nx.add(win_size)
    |> Nx.reshape({:auto, 1})
    |> Nx.slice([0, 0], [p - win_size, 1])

  x =
    Nx.slice(series, [x_idx[0]], [win_size])
    |> Nx.devectorize(keep_names: false)
    |> Nx.slice([0, 0], [p - win_size, win_size])

  y = Nx.gather(series, y_idx, axes: [0])

  {x, y}
end
```

<!-- livebook:{"output":true} -->

```
#Function<41.125776118/2 in :erl_eval.expr/6>
```

```elixir
test_series = Nx.tensor([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  s64[10]
  EXLA.Backend<host:0, 0.2552156563.1001783322.58502>
  [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]
>
```

```elixir
sliding_window.(test_series, 2)
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   s64[8][2]
   EXLA.Backend<host:0, 0.2552156563.1001783310.45295>
   [
     [0, 1],
     [1, 1],
     [1, 2],
     [2, 3],
     [3, 5],
     [5, 8],
     [8, 13],
     [13, 21]
   ]
 >,
 #Nx.Tensor<
   s64[8]
   EXLA.Backend<host:0, 0.2552156563.1001783310.45297>
   [1, 2, 3, 5, 8, 13, 21, 34]
 >}
```

```elixir
p = 133
win_size = 7
trunc_data = data[0..(p - 1)]

{x, y} = sliding_window.(trunc_data, win_size)
```

<!-- livebook:{"output":true} -->

```
{#Nx.Tensor<
   f32[126][7]
   EXLA.Backend<host:0, 0.2552156563.1001783310.45359>
   [
     [-0.7006233930587769, -0.8208848237991333, -0.9393830299377441, -0.9471651911735535, -0.6878552436828613, -0.8432590365409851, -0.8053202033042908],
     [-0.8208848237991333, -0.9393830299377441, -0.9471651911735535, -0.6878552436828613, -0.8432590365409851, -0.8053202033042908, -0.8205807209014893],
     [-0.9393830299377441, -0.9471651911735535, -0.6878552436828613, -0.8432590365409851, -0.8053202033042908, -0.8205807209014893, -0.9202312231063843],
     [-0.9471651911735535, -0.6878552436828613, -0.8432590365409851, -0.8053202033042908, -0.8205807209014893, -0.9202312231063843, -1.0],
     [-0.6878552436828613, -0.8432590365409851, -0.8053202033042908, -0.8205807209014893, -0.9202312231063843, -1.0, -0.9881443977355957],
     [-0.8432590365409851, -0.8053202033042908, -0.8205807209014893, -0.9202312231063843, -1.0, -0.9881443977355957, -0.8596141338348389],
     [-0.8053202033042908, -0.8205807209014893, -0.9202312231063843, -1.0, -0.9881443977355957, -0.8596141338348389, -0.8706188201904297],
     ...
   ]
 >,
 #Nx.Tensor<
   f32[126]
   EXLA.Backend<host:0, 0.2552156563.1001783310.45361>
   [-0.8205807209014893, -0.9202312231063843, -1.0, -0.9881443977355957, -0.8596141338348389, -0.8706188201904297, -0.9266151189804077, -0.8011858463287354, -0.7628820538520813, -0.6649947762489319, -0.6728988289833069, -0.6822011470794678, -0.542119026184082, -0.4650859236717224, -0.21489591896533966, -0.17020823061466217, -0.08247456699609756, 0.06411336362361908, 0.08575760573148727, 0.38604652881622314, 0.39468035101890564, 0.40708330273628235, 0.5548260807991028, 0.45712119340896606, 0.2172670066356659, 0.3825809359550476, 0.16187873482704163, 0.16838431358337402, -0.002279982203617692, 0.21903042495250702, 0.16187873482704163, 0.321294903755188, 0.21939483284950256, 0.2579419016838074, 0.3031162619590759, 0.4281805753707886, 0.42708620429039, 0.3619089424610138, 0.34075120091438293, 0.5010794997215271, 0.5381670594215393, 0.7000153660774231, 0.8822922110557556, 0.7957746386528015, 0.885089099407196, 1.0, 0.9240614771842957, 0.828606128692627, ...]
 >}
```

## Train-test splitting

```elixir
train_test_split = 2 / 3

{x_train, x_test} = Nx.split(x, train_test_split)
{y_train, y_test} = Nx.split(y, train_test_split)

# reshape for LSTM layer input
batch_size = 8
```

<!-- livebook:{"output":true} -->

```
8
```

## Building and training our RNN

```elixir
input_layer = Axon.input("input", shape: {batch_size, 7, 1})
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"input" => {8, 7, 1}}
  outputs: "input"
  nodes: 1
>
```

```elixir
model =
  input_layer
  |> Axon.lstm(5, name: "LSTM")
  |> then(fn {out, _} -> out end)
  # takes the last element of the sequence
  |> Axon.nx(fn t -> t[[0..-1//1, -1]] end)
  |> Axon.dense(1)

Axon.Display.as_graph(model, Nx.template({batch_size, 7, 1}, :f32))
```

<!-- livebook:{"output":true} -->

```mermaid
graph TD;
11[/"input (:input) {8, 7, 1}"/];
12["LSTM_c_hidden_state (:recurrent_state) {8, 5}"];
13["LSTM_h_hidden_state (:recurrent_state) {8, 5}"];
14["constant_0 (:constant) {}"];
15["LSTM_hidden_state (:container) {{8, 5}, {8, 5}}"];
16["LSTM (:lstm) {{8, 7, 5}, {{8, 5}, {8, 5}}}"];
17["LSTM_output_sequence (:elem) {8, 7, 5}"];
20["nx_0 (:nx) {8, 5}"];
21["dense_0 (:dense) {8, 1}"];
20 --> 21;
17 --> 20;
16 --> 17;
14 --> 16;
15 --> 16;
11 --> 16;
13 --> 15;
12 --> 15;
11 --> 13;
11 --> 12;
```

```elixir
optimizer = Polaris.Optimizers.sgd(learning_rate: 0.05)

loop = Axon.Loop.trainer(model, :mean_squared_error, optimizer)

y_train_batches = Nx.to_batched(y_train, batch_size)

x_train_batches =
  x_train
  |> Nx.reshape({:auto, win_size, 1})
  |> Nx.to_batched(batch_size)

y_test_batches = Nx.to_batched(y_test, batch_size)

x_test_batches =
  x_test
  |> Nx.reshape({:auto, win_size, 1})
  |> Nx.to_batched(batch_size)

train_data = Stream.zip([x_train_batches, y_train_batches])
trained_model_state = Axon.Loop.run(loop, train_data, %{}, epochs: 1000)
```

<!-- livebook:{"output":true} -->

```
...



Epoch: 4, Batch: 6, loss: 0.1406003




Epoch: 9, Batch: 1, loss: 0.1055254



Epoch: 13, Batch: 7, loss: 0.0920379




Epoch: 18, Batch: 2, loss: 0.0840348



Epoch: 22, Batch: 8, loss: 0.0797941




Epoch: 27, Batch: 3, loss: 0.0765455



Epoch: 31, Batch: 9, loss: 0.0742033




Epoch: 36, Batch: 4, loss: 0.0723929



Epoch: 40, Batch: 10, loss: 0.0709493




Epoch: 45, Batch: 5, loss: 0.0697711




Epoch: 50, Batch: 0, loss: 0.0688711



Epoch: 54, Batch: 6, loss: 0.0681244




Epoch: 59, Batch: 1, loss: 0.0673259



Epoch: 63, Batch: 7, loss: 0.0668674




Epoch: 68, Batch: 2, loss: 0.0662318



Epoch: 72, Batch: 8, loss: 0.0658728




Epoch: 77, Batch: 3, loss: 0.0654387



Epoch: 81, Batch: 9, loss: 0.0650305




Epoch: 86, Batch: 4, loss: 0.0646709



Epoch: 90, Batch: 10, loss: 0.0643204




Epoch: 95, Batch: 5, loss: 0.0640090




Epoch: 100, Batch: 0, loss: 0.0637516



Epoch: 104, Batch: 6, loss: 0.0635178




Epoch: 109, Batch: 1, loss: 0.0632290



Epoch: 113, Batch: 7, loss: 0.0630820




Epoch: 118, Batch: 2, loss: 0.0628160



Epoch: 122, Batch: 8, loss: 0.0626814




Epoch: 127, Batch: 3, loss: 0.0624914



Epoch: 131, Batch: 9, loss: 0.0622998




Epoch: 136, Batch: 4, loss: 0.0621271



Epoch: 140, Batch: 10, loss: 0.0619490




Epoch: 145, Batch: 5, loss: 0.0617864




Epoch: 150, Batch: 0, loss: 0.0616528



Epoch: 154, Batch: 6, loss: 0.0615237




Epoch: 159, Batch: 1, loss: 0.0613589



Epoch: 163, Batch: 7, loss: 0.0612809




Epoch: 168, Batch: 2, loss: 0.0611206



Epoch: 172, Batch: 8, loss: 0.0610439




Epoch: 177, Batch: 3, loss: 0.0609276



Epoch: 181, Batch: 9, loss: 0.0608070




Epoch: 186, Batch: 4, loss: 0.0606970



Epoch: 190, Batch: 10, loss: 0.0605814




Epoch: 195, Batch: 5, loss: 0.0604739




Epoch: 200, Batch: 0, loss: 0.0603874



Epoch: 204, Batch: 6, loss: 0.0602993




Epoch: 209, Batch: 1, loss: 0.0601870



Epoch: 213, Batch: 7, loss: 0.0601359




Epoch: 218, Batch: 2, loss: 0.0600237



Epoch: 222, Batch: 8, loss: 0.0599715




Epoch: 227, Batch: 3, loss: 0.0598894



Epoch: 231, Batch: 9, loss: 0.0598029




Epoch: 236, Batch: 4, loss: 0.0597234



Epoch: 240, Batch: 10, loss: 0.0596393




Epoch: 245, Batch: 5, loss: 0.0595600




Epoch: 250, Batch: 0, loss: 0.0594977



Epoch: 254, Batch: 6, loss: 0.0594314




Epoch: 259, Batch: 1, loss: 0.0593477



Epoch: 263, Batch: 7, loss: 0.0593106




Epoch: 268, Batch: 2, loss: 0.0592256



Epoch: 272, Batch: 8, loss: 0.0591867




Epoch: 277, Batch: 3, loss: 0.0591243



Epoch: 281, Batch: 9, loss: 0.0590577




Epoch: 286, Batch: 4, loss: 0.0589964



Epoch: 290, Batch: 10, loss: 0.0589312




Epoch: 295, Batch: 5, loss: 0.0588691




Epoch: 300, Batch: 0, loss: 0.0588215



Epoch: 304, Batch: 6, loss: 0.0587686




Epoch: 309, Batch: 1, loss: 0.0587030



Epoch: 313, Batch: 7, loss: 0.0586745




Epoch: 318, Batch: 2, loss: 0.0586071



Epoch: 322, Batch: 8, loss: 0.0585767




Epoch: 327, Batch: 3, loss: 0.0585270



Epoch: 331, Batch: 9, loss: 0.0584735




Epoch: 336, Batch: 4, loss: 0.0584242



Epoch: 340, Batch: 10, loss: 0.0583716




Epoch: 345, Batch: 5, loss: 0.0583212




Epoch: 350, Batch: 0, loss: 0.0582834



Epoch: 354, Batch: 6, loss: 0.0582398




Epoch: 359, Batch: 1, loss: 0.0581865



Epoch: 363, Batch: 7, loss: 0.0581639




Epoch: 368, Batch: 2, loss: 0.0581087



Epoch: 372, Batch: 8, loss: 0.0580841




Epoch: 377, Batch: 3, loss: 0.0580434



Epoch: 381, Batch: 9, loss: 0.0579992




Epoch: 386, Batch: 4, loss: 0.0579585



Epoch: 390, Batch: 10, loss: 0.0579149




Epoch: 395, Batch: 5, loss: 0.0578729




Epoch: 400, Batch: 0, loss: 0.0578420



Epoch: 404, Batch: 6, loss: 0.0578053




Epoch: 409, Batch: 1, loss: 0.0577610



Epoch: 413, Batch: 7, loss: 0.0577426




Epoch: 418, Batch: 2, loss: 0.0576964



Epoch: 422, Batch: 8, loss: 0.0576761




Epoch: 427, Batch: 3, loss: 0.0576421



Epoch: 431, Batch: 9, loss: 0.0576048




Epoch: 436, Batch: 4, loss: 0.0575705



Epoch: 440, Batch: 10, loss: 0.0575336




Epoch: 445, Batch: 5, loss: 0.0574980




Epoch: 450, Batch: 0, loss: 0.0574723



Epoch: 454, Batch: 6, loss: 0.0574409




Epoch: 459, Batch: 1, loss: 0.0574033



Epoch: 463, Batch: 7, loss: 0.0573881




Epoch: 468, Batch: 2, loss: 0.0573488



Epoch: 472, Batch: 8, loss: 0.0573319




Epoch: 477, Batch: 3, loss: 0.0573030



Epoch: 481, Batch: 9, loss: 0.0572709




Epoch: 486, Batch: 4, loss: 0.0572416



Epoch: 490, Batch: 10, loss: 0.0572099




Epoch: 495, Batch: 5, loss: 0.0571793




Epoch: 500, Batch: 0, loss: 0.0571575



Epoch: 504, Batch: 6, loss: 0.0571302




Epoch: 509, Batch: 1, loss: 0.0570979



Epoch: 513, Batch: 7, loss: 0.0570852




Epoch: 518, Batch: 2, loss: 0.0570513



Epoch: 522, Batch: 8, loss: 0.0570369




Epoch: 527, Batch: 3, loss: 0.0570121



Epoch: 531, Batch: 9, loss: 0.0569842




Epoch: 536, Batch: 4, loss: 0.0569589



Epoch: 540, Batch: 10, loss: 0.0569313




Epoch: 545, Batch: 5, loss: 0.0569046




Epoch: 550, Batch: 0, loss: 0.0568860



Epoch: 554, Batch: 6, loss: 0.0568620




Epoch: 559, Batch: 1, loss: 0.0568339



Epoch: 563, Batch: 7, loss: 0.0568231




Epoch: 568, Batch: 2, loss: 0.0567935



Epoch: 572, Batch: 8, loss: 0.0567812




Epoch: 577, Batch: 3, loss: 0.0567596



Epoch: 581, Batch: 9, loss: 0.0567351




Epoch: 586, Batch: 4, loss: 0.0567129



Epoch: 590, Batch: 10, loss: 0.0566887




Epoch: 595, Batch: 5, loss: 0.0566651




Epoch: 600, Batch: 0, loss: 0.0566490



Epoch: 604, Batch: 6, loss: 0.0566278




Epoch: 609, Batch: 1, loss: 0.0566031



Epoch: 613, Batch: 7, loss: 0.0565940




Epoch: 618, Batch: 2, loss: 0.0565678



Epoch: 622, Batch: 8, loss: 0.0565573




Epoch: 627, Batch: 3, loss: 0.0565382



Epoch: 631, Batch: 9, loss: 0.0565165




Epoch: 636, Batch: 4, loss: 0.0564969



Epoch: 640, Batch: 10, loss: 0.0564753




Epoch: 645, Batch: 5, loss: 0.0564544




Epoch: 650, Batch: 0, loss: 0.0564404



Epoch: 654, Batch: 6, loss: 0.0564214




Epoch: 659, Batch: 1, loss: 0.0563994



Epoch: 663, Batch: 7, loss: 0.0563916




Epoch: 668, Batch: 2, loss: 0.0563683



Epoch: 672, Batch: 8, loss: 0.0563592




Epoch: 677, Batch: 3, loss: 0.0563423



Epoch: 681, Batch: 9, loss: 0.0563228




Epoch: 686, Batch: 4, loss: 0.0563054



Epoch: 690, Batch: 10, loss: 0.0562860




Epoch: 695, Batch: 5, loss: 0.0562674




Epoch: 700, Batch: 0, loss: 0.0562549



Epoch: 704, Batch: 6, loss: 0.0562379




Epoch: 709, Batch: 1, loss: 0.0562182



Epoch: 713, Batch: 7, loss: 0.0562114




Epoch: 718, Batch: 2, loss: 0.0561905



Epoch: 722, Batch: 8, loss: 0.0561825




Epoch: 727, Batch: 3, loss: 0.0561674



Epoch: 731, Batch: 9, loss: 0.0561498




Epoch: 736, Batch: 4, loss: 0.0561342



Epoch: 740, Batch: 10, loss: 0.0561167




Epoch: 745, Batch: 5, loss: 0.0560999




Epoch: 750, Batch: 0, loss: 0.0560888



Epoch: 754, Batch: 6, loss: 0.0560734




Epoch: 759, Batch: 1, loss: 0.0560556



Epoch: 763, Batch: 7, loss: 0.0560497




Epoch: 768, Batch: 2, loss: 0.0560308



Epoch: 772, Batch: 8, loss: 0.0560237




Epoch: 777, Batch: 3, loss: 0.0560101



Epoch: 781, Batch: 9, loss: 0.0559942




Epoch: 786, Batch: 4, loss: 0.0559801



Epoch: 790, Batch: 10, loss: 0.0559641




Epoch: 795, Batch: 5, loss: 0.0559488




Epoch: 800, Batch: 0, loss: 0.0559389



Epoch: 804, Batch: 6, loss: 0.0559249




Epoch: 809, Batch: 1, loss: 0.0559087



Epoch: 813, Batch: 7, loss: 0.0559035




Epoch: 818, Batch: 2, loss: 0.0558863



Epoch: 822, Batch: 8, loss: 0.0558801




Epoch: 827, Batch: 3, loss: 0.0558677



Epoch: 831, Batch: 9, loss: 0.0558532




Epoch: 836, Batch: 4, loss: 0.0558403



Epoch: 840, Batch: 10, loss: 0.0558257




Epoch: 845, Batch: 5, loss: 0.0558117




Epoch: 850, Batch: 0, loss: 0.0558028



Epoch: 854, Batch: 6, loss: 0.0557899




Epoch: 859, Batch: 1, loss: 0.0557751



Epoch: 863, Batch: 7, loss: 0.0557706




Epoch: 868, Batch: 2, loss: 0.0557548



Epoch: 872, Batch: 8, loss: 0.0557493




Epoch: 877, Batch: 3, loss: 0.0557381



Epoch: 881, Batch: 9, loss: 0.0557247




Epoch: 886, Batch: 4, loss: 0.0557129



Epoch: 890, Batch: 10, loss: 0.0556995




Epoch: 895, Batch: 5, loss: 0.0556867




Epoch: 900, Batch: 0, loss: 0.0556785



Epoch: 904, Batch: 6, loss: 0.0556667




Epoch: 909, Batch: 1, loss: 0.0556531



Epoch: 913, Batch: 7, loss: 0.0556490




Epoch: 918, Batch: 2, loss: 0.0556346



Epoch: 922, Batch: 8, loss: 0.0556295




Epoch: 927, Batch: 3, loss: 0.0556192



Epoch: 931, Batch: 9, loss: 0.0556068




Epoch: 936, Batch: 4, loss: 0.0555960



Epoch: 940, Batch: 10, loss: 0.0555836




Epoch: 945, Batch: 5, loss: 0.0555717




Epoch: 950, Batch: 0, loss: 0.0555643



Epoch: 954, Batch: 6, loss: 0.0555534




Epoch: 959, Batch: 1, loss: 0.0555407



Epoch: 963, Batch: 7, loss: 0.0555371




Epoch: 968, Batch: 2, loss: 0.0555237



Epoch: 972, Batch: 8, loss: 0.0555192




Epoch: 977, Batch: 3, loss: 0.0555097



Epoch: 981, Batch: 9, loss: 0.0554982




Epoch: 986, Batch: 4, loss: 0.0554883



Epoch: 990, Batch: 10, loss: 0.0554767




Epoch: 995, Batch: 5, loss: 0.0554658




```

<!-- livebook:{"output":true} -->

```
%{
  "LSTM" => %{
    "bias" => %{
      "bf" => #Nx.Tensor<
        f32[5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234256>
        [-0.002171213971450925, -0.10862083733081818, 0.013676775619387627, -0.028514347970485687, -0.1776691973209381]
      >,
      "bg" => #Nx.Tensor<
        f32[5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234257>
        [-0.007562990766018629, -0.19760337471961975, 0.07507184892892838, 0.018798265606164932, 0.2767767608165741]
      >,
      "bi" => #Nx.Tensor<
        f32[5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234258>
        [-0.007146107964217663, 0.4131268262863159, -0.06548699736595154, 0.019385823979973793, 0.257880300283432]
      >,
      "bo" => #Nx.Tensor<
        f32[5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234259>
        [-0.011299226433038712, 0.35287508368492126, -0.027267329394817352, -0.029471075162291527, 0.07989538460969925]
      >
    },
    "hidden_kernel" => %{
      "whf" => #Nx.Tensor<
        f32[5][5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234260>
        [
          [0.0010181638645008206, 0.5000221729278564, 0.6709553599357605, -0.18457747995853424, 0.5969188809394836],
          [0.027324775233864784, -0.10921439528465271, -0.004526784177869558, -0.6615952253341675, -0.33729034662246704],
          [-0.7112587690353394, 0.6282284259796143, -0.41691452264785767, 0.16429704427719116, -0.4601139426231384],
          [0.509546160697937, -0.3933703303337097, -0.6547205448150635, -0.7275545001029968, -0.5100501179695129],
          [-0.648013710975647, -0.5393060445785522, 0.19414305686950684, 0.0988604724407196, -0.41855770349502563]
        ]
      >,
      "whg" => #Nx.Tensor<
        f32[5][5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234261>
        [
          [0.5345804691314697, 0.07505007833242416, -0.32909485697746277, -0.19112418591976166, 0.3670359253883362],
          [0.4478081464767456, -0.9449050426483154, -0.5069870948791504, 0.5481019616127014, -0.5542922616004944],
          [0.6324222087860107, -0.11484222859144211, -0.45115235447883606, -0.18387162685394287, -0.6101811528205872],
          [-0.2097945362329483, -0.3311927318572998, 0.1557314395904541, 0.2923789620399475, 0.3255537450313568],
          [0.01836143061518669, 0.5230696797370911, 0.6491124033927917, 0.5792794823646545, -0.9265835881233215]
        ]
      >,
      "whi" => #Nx.Tensor<
        f32[5][5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234262>
        [
          [-0.06308569759130478, 0.3548959493637085, 0.3832937479019165, 0.08306227624416351, 0.602133572101593],
          [0.41725704073905945, -0.015052979812026024, -0.5012054443359375, 0.6567822098731995, -0.5310749411582947],
          [-0.43583083152770996, -0.19516099989414215, 0.2694666087627411, -0.6977806091308594, 0.05481889843940735],
          [-0.6075629591941833, 0.7113666534423828, -0.44857916235923767, -0.38321515917778015, -0.64043790102005],
          [-0.4028346538543701, 0.7444376349449158, -0.24044543504714966, 0.7564346790313721, 0.04126371815800667]
        ]
      >,
      "who" => #Nx.Tensor<
        f32[5][5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234263>
        [
          [-0.12942585349082947, -0.5717195868492126, -0.07771812379360199, -0.17744335532188416, 0.7260860800743103],
          [0.6877514123916626, 0.07122814655303955, -0.5406115651130676, -0.2592248022556305, 0.7166458964347839],
          [0.08924061805009842, -0.5544683933258057, 0.6197776198387146, 0.24718573689460754, 0.1461210548877716],
          [-0.044443100690841675, 0.7865015864372253, 0.4346669912338257, 0.4628666937351227, 0.7941024899482727],
          [-0.2807670533657074, -0.08812703937292099, -0.16225071251392365, 0.06780480593442917, 0.8028830885887146]
        ]
      >
    },
    "input_kernel" => %{
      "wif" => #Nx.Tensor<
        f32[1][5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234264>
        [
          [-0.16475994884967804, -0.5157750844955444, 0.7031154632568359, -0.37493112683296204, 0.2450779229402542]
        ]
      >,
      "wig" => #Nx.Tensor<
        f32[1][5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234265>
        [
          [0.1432686597108841, 1.2649250030517578, -0.60526043176651, -0.59303879737854, -0.8276455998420715]
        ]
      >,
      "wii" => #Nx.Tensor<
        f32[1][5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234266>
        [
          [0.6687301993370056, -0.5023969411849976, -0.42522892355918884, -0.2477462738752365, -0.07878562808036804]
        ]
      >,
      "wio" => #Nx.Tensor<
        f32[1][5]
        EXLA.Backend<host:0, 0.2552156563.1002045454.234267>
        [
          [-0.651384711265564, 0.19540289044380188, 0.3278134763240814, 0.6693854928016663, -0.5902070999145508]
        ]
      >
    }
  },
  "LSTM_c_hidden_state" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.2552156563.1002045454.234268>
      [400762965, 3904035632]
    >
  },
  "LSTM_h_hidden_state" => %{
    "key" => #Nx.Tensor<
      u32[2]
      EXLA.Backend<host:0, 0.2552156563.1002045454.234269>
      [400762965, 3904035632]
    >
  },
  "dense_0" => %{
    "bias" => #Nx.Tensor<
      f32[1]
      EXLA.Backend<host:0, 0.2552156563.1002045454.234270>
      [0.18337668478488922]
    >,
    "kernel" => #Nx.Tensor<
      f32[5][1]
      EXLA.Backend<host:0, 0.2552156563.1002045454.234271>
      [
        [-0.01825246959924698],
        [1.5190180540084839],
        [0.31094667315483093],
        [-0.15878179669380188],
        [-1.0259007215499878]
      ]
    >
  }
}
```

```elixir
y_pred_train =
  x_train_batches
  |> Enum.map(&Axon.predict(model, trained_model_state, &1))
  |> Nx.concatenate()

y_pred_test =
  x_test_batches
  |> Enum.map(&Axon.predict(model, trained_model_state, &1))
  |> Nx.concatenate()
```

<!-- livebook:{"output":true} -->

```
#Nx.Tensor<
  f32[48][1]
  EXLA.Backend<host:0, 0.2552156563.1002045454.234316>
  [
    [-0.5447931289672852],
    [-0.605151891708374],
    [-0.6833282709121704],
    [-0.7301497459411621],
    [-0.6993919610977173],
    [-0.6826620101928711],
    [-0.6617048382759094],
    [-0.6146981716156006],
    [-0.4872749447822571],
    [-0.4967283010482788],
    [-0.24390225112438202],
    [-0.22310416400432587],
    [-0.2611315846443176],
    [-0.20334573090076447],
    [-0.41010046005249023],
    [-0.2774851322174072],
    [-0.33974164724349976],
    [-0.3135749101638794],
    [-0.28247499465942383],
    [-0.16546089947223663],
    [-0.0464247465133667],
    [-0.08019264042377472],
    [-0.09809987246990204],
    [-0.06863798201084137],
    [-0.07820086181163788],
    [0.12704670429229736],
    [0.17152218520641327],
    [0.1300748884677887],
    [0.19346946477890015],
    [0.14522811770439148],
    [0.021727174520492554],
    [-0.031376913189888],
    [0.03356589376926422],
    [0.09383520483970642],
    [-0.15192650258541107],
    [-0.11447562277317047],
    [0.05846433341503143],
    [-0.05228118598461151],
    [-0.07752542197704315],
    [-0.04644341766834259],
    [-0.038404062390327454],
    [0.019303783774375916],
    [-0.5437518954277039],
    [-0.604383647441864],
    [-0.6914800405502319],
    [-0.7342785000801086],
    [-0.7047759890556335],
    [-0.6837592720985413]
  ]
>
```

## Visualizing predictions

```elixir
plt_data = plt_data ++ [eval: List.duplicate("real prices", 138)]

data_eval_train = [
  prices: Nx.to_flat_list(y_pred_train),
  t: Nx.linspace(0, 87, n: 84, type: {:u, 8}),
  eval: List.duplicate("predictions train", 88)
]

data_eval_test = [
  prices: Nx.to_flat_list(y_pred_test),
  t: Nx.linspace(88, 137, n: 42, type: {:u, 8}),
  eval: List.duplicate("predictions test", 48)
]

data_opts = [height: 400, width: 600, stroke_dash: [2], stroke_width: 2]
train_opts = [height: 400, width: 600, stroke_width: 2]
test_opts = [height: 400, width: 600, stroke_width: 2]

Tucan.layers([
  Tucan.lineplot(plt_data, "t", "prices", data_opts),
  Tucan.lineplot(data_eval_train, "t", "prices", train_opts),
  Tucan.lineplot(data_eval_test, "t", "prices", test_opts)
])
|> Tucan.color_by("eval")
|> Tucan.set_theme(:urban_institute)

# Comment to visualize with KinoVegaLite
:ok
```

<!-- livebook:{"output":true} -->

```
:ok
```

![](files/predictions.png)

```elixir
mse = fn y_pred, y_true ->
  Axon.Losses.mean_squared_error(y_pred, y_true, reduction: :mean)
end

train_loop =
  model
  |> Axon.Loop.evaluator()
  |> Axon.Loop.metric(mse, "mse")

Axon.Loop.run(train_loop, train_data, trained_model_state, epochs: 100)
```

<!-- livebook:{"output":true} -->

```
Batch: 10, mse: 0.0523675
```

<!-- livebook:{"output":true} -->

```
%{
  93 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.256080>
      0.052367452532052994
    >
  },
  78 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.252615>
      0.052367452532052994
    >
  },
  15 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.238062>
      0.052367452532052994
    >
  },
  31 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.241758>
      0.052367452532052994
    >
  },
  16 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.238293>
      0.052367452532052994
    >
  },
  86 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.254463>
      0.052367452532052994
    >
  },
  34 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.242451>
      0.052367452532052994
    >
  },
  83 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.253770>
      0.052367452532052994
    >
  },
  99 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.257466>
      0.052367452532052994
    >
  },
  38 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.243375>
      0.052367452532052994
    >
  },
  49 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.245916>
      0.052367452532052994
    >
  },
  27 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.240834>
      0.052367452532052994
    >
  },
  52 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.246609>
      0.052367452532052994
    >
  },
  37 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.243144>
      0.052367452532052994
    >
  },
  41 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.244068>
      0.052367452532052994
    >
  },
  81 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.253308>
      0.052367452532052994
    >
  },
  75 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.251922>
      0.052367452532052994
    >
  },
  80 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.253077>
      0.052367452532052994
    >
  },
  61 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.248688>
      0.052367452532052994
    >
  },
  91 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.255618>
      0.052367452532052994
    >
  },
  66 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.249843>
      0.052367452532052994
    >
  },
  97 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.257004>
      0.052367452532052994
    >
  },
  55 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.247302>
      0.052367452532052994
    >
  },
  6 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.235983>
      0.052367452532052994
    >
  },
  67 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.250074>
      0.052367452532052994
    >
  },
  5 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.235752>
      0.052367452532052994
    >
  },
  14 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.237831>
      0.052367452532052994
    >
  },
  90 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.255387>
      0.052367452532052994
    >
  },
  30 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.241527>
      0.052367452532052994
    >
  },
  74 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.251691>
      0.052367452532052994
    >
  },
  82 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.253539>
      0.052367452532052994
    >
  },
  17 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.238524>
      0.052367452532052994
    >
  },
  20 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.239217>
      0.052367452532052994
    >
  },
  58 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.247995>
      0.052367452532052994
    >
  },
  32 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.241989>
      0.052367452532052994
    >
  },
  73 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.251460>
      0.052367452532052994
    >
  },
  21 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.239448>
      0.052367452532052994
    >
  },
  96 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.256773>
      0.052367452532052994
    >
  },
  88 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.254925>
      0.052367452532052994
    >
  },
  33 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.242220>
      0.052367452532052994
    >
  },
  60 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.248457>
      0.052367452532052994
    >
  },
  57 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.247764>
      0.052367452532052994
    >
  },
  2 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.235059>
      0.052367452532052994
    >
  },
  35 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.242682>
      0.052367452532052994
    >
  },
  0 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.234597>
      0.052367452532052994
    >
  },
  79 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.252846>
      0.052367452532052994
    >
  },
  84 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.254001>
      0.052367452532052994
    >
  },
  19 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.238986>
      0.052367452532052994
    >
  },
  44 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.244761>
      0.052367452532052994
    >
  },
  22 => %{...},
  ...
}
```

```elixir
test_data = Stream.zip([x_test_batches, y_test_batches])

test_loop =
  model
  |> Axon.Loop.evaluator()
  |> Axon.Loop.metric(mse, "mse")

Axon.Loop.run(test_loop, test_data, trained_model_state, epochs: 100)
```

<!-- livebook:{"output":true} -->

```
Batch: 5, mse: 0.0463733
```

<!-- livebook:{"output":true} -->

```
%{
  93 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.7188>
      0.04637334123253822
    >
  },
  78 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.5298>
      0.04637334123253822
    >
  },
  15 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.259504>
      0.04637334123253822
    >
  },
  31 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.261520>
      0.04637334123253822
    >
  },
  16 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.259630>
      0.04637334123253822
    >
  },
  86 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.6306>
      0.04637334123253822
    >
  },
  34 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.261898>
      0.04637334123253822
    >
  },
  83 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.5928>
      0.04637334123253822
    >
  },
  99 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.7944>
      0.04637334123253822
    >
  },
  38 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.258>
      0.04637334123253822
    >
  },
  49 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.1644>
      0.04637334123253822
    >
  },
  27 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.261016>
      0.04637334123253822
    >
  },
  52 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.2022>
      0.04637334123253822
    >
  },
  37 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.132>
      0.04637334123253822
    >
  },
  41 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.636>
      0.04637334123253822
    >
  },
  81 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.5676>
      0.04637334123253822
    >
  },
  75 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.4920>
      0.04637334123253822
    >
  },
  80 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.5550>
      0.04637334123253822
    >
  },
  61 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.3156>
      0.04637334123253822
    >
  },
  91 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.6936>
      0.04637334123253822
    >
  },
  66 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.3786>
      0.04637334123253822
    >
  },
  97 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.7692>
      0.04637334123253822
    >
  },
  55 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.2400>
      0.04637334123253822
    >
  },
  6 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.258370>
      0.04637334123253822
    >
  },
  67 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.3912>
      0.04637334123253822
    >
  },
  5 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.258244>
      0.04637334123253822
    >
  },
  14 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.259378>
      0.04637334123253822
    >
  },
  90 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.6810>
      0.04637334123253822
    >
  },
  30 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.261394>
      0.04637334123253822
    >
  },
  74 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.4794>
      0.04637334123253822
    >
  },
  82 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.5802>
      0.04637334123253822
    >
  },
  17 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.259756>
      0.04637334123253822
    >
  },
  20 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.260134>
      0.04637334123253822
    >
  },
  58 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.2778>
      0.04637334123253822
    >
  },
  32 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.261646>
      0.04637334123253822
    >
  },
  73 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.4668>
      0.04637334123253822
    >
  },
  21 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.260260>
      0.04637334123253822
    >
  },
  96 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.7566>
      0.04637334123253822
    >
  },
  88 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.6558>
      0.04637334123253822
    >
  },
  33 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.261772>
      0.04637334123253822
    >
  },
  60 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.3030>
      0.04637334123253822
    >
  },
  57 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.2652>
      0.04637334123253822
    >
  },
  2 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.257866>
      0.04637334123253822
    >
  },
  35 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.262024>
      0.04637334123253822
    >
  },
  0 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.257614>
      0.04637334123253822
    >
  },
  79 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.5424>
      0.04637334123253822
    >
  },
  84 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.6054>
      0.04637334123253822
    >
  },
  19 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002045454.260008>
      0.04637334123253822
    >
  },
  44 => %{
    "mse" => #Nx.Tensor<
      f32
      EXLA.Backend<host:0, 0.2552156563.1002307598.1014>
      0.04637334123253822
    >
  },
  22 => %{...},
  ...
}
```
